{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0adac685",
   "metadata": {},
   "source": [
    "# Preprocessing Text  \n",
    "Terdapat 7 tahapan dalam preprocessing text yakni:  \n",
    "1. Case Folding  \n",
    "2. Remove Special Characters\n",
    "3. Menghapus Angka\n",
    "4. Menghapus Tanda Baca\n",
    "5. Typo Correction\n",
    "6. Tokenizing \n",
    "7. Stopword\n",
    "8. Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84572fb6",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "619b300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ANDIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ANDIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ANDIK\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from spellchecker import SpellChecker\n",
    "import emoji\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c668cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15000 entries, 0 to 14999\n",
      "Data columns (total 2 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   content  15000 non-null  object\n",
      " 1   score    15000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 234.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('data/data.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b69c6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi SpellChecker untuk Bahasa Inggris\n",
    "spell = SpellChecker()\n",
    "\n",
    "# Fungsi untuk spell checking menggunakan pyspellchecker\n",
    "def correct_text_spellchecker(text):\n",
    "    words = text.split()\n",
    "    corrected_words = [spell.correction(word) for word in words]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Fungsi untuk normalisasi menggunakan kamus custom\n",
    "def normalize_custom_kamus(text, kamus_dict):\n",
    "    return ' '.join([kamus_dict.get(word, word) for word in text.split()])\n",
    "\n",
    "# Fungsi untuk menghapus emoji\n",
    "def remove_emoji(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "\n",
    "# --- Fungsi Preprocessing Lengkap ---\n",
    "def preprocess(text, kamus_custom):\n",
    "    # 6. Normalisasi Kata (Menggunakan Kamus Custom)\n",
    "    text = normalize_custom_kamus(text, kamus_custom)\n",
    "    \n",
    "    # 1. Case Folding\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove Emoji\n",
    "    text = remove_emoji(text)\n",
    "\n",
    "    # 3. Remove Special Characters\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # 4. Menghapus Angka\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # 5. Menghapus Tanda Baca\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # 6. Normalisasi Kata (Menggunakan Kamus Custom)\n",
    "    text = normalize_custom_kamus(text, kamus_custom)\n",
    "\n",
    "    # 7. Spell Checking\n",
    "    text = correct_text_spellchecker(text)\n",
    "    \n",
    "    # 8. Tokenizing\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 9. Stopword Removal\n",
    "    stopword_list = stopwords.words('english')\n",
    "    tokens = [word for word in tokens if word not in stopword_list]\n",
    "    \n",
    "    # 10. Lemmatization \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Fungsi untuk memuat kamus custom dari file\n",
    "def load_custom_dictionary(filepath):\n",
    "    custom_dict = {}\n",
    "    with open(filepath, encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            if '=' in line:\n",
    "                key, value = line.strip().split('=', 1)\n",
    "                custom_dict[key.strip()] = value.strip()\n",
    "    return custom_dict\n",
    "\n",
    "# Load kamus custom\n",
    "kamus_custom = load_custom_dictionary('kamus_custom_en.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d0c991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game amazing please fix lag please laughing loud league legend laughing loud laughing loud thanks thanks thanks\n"
     ]
    }
   ],
   "source": [
    "ulasan = \"this game is amazing pls fix the lag pls lol LoL Lol LOL thx Thx THX\"\n",
    "# ulasan = correct_text_spellchecker(ulasan)\n",
    "# print(ulasan)\n",
    "processed_text = preprocess(ulasan, kamus_custom)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed3d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the first 10 rows of the 'content' column\n",
    "data_sample = data.iloc[:10].copy()  # Copy the first 10 rows for preprocessing\n",
    "data_sample['processed_content'] = data_sample['content'].apply(preprocess)\n",
    "\n",
    "# Display the processed content one by one\n",
    "for index, row in data_sample.iterrows():\n",
    "    print(f\"Row {index}: {row['processed_content']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f409225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0db69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membaca file CSV\n",
    "data = pd.read_csv('data/data.csv')\n",
    "\n",
    "# Looping untuk mencetak isi kolom 'content'\n",
    "for index, content in enumerate(data['content']):\n",
    "    print(f\"Row {index}: {content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741bb7b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentimen_analisis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
